---
phase: 05-debt-closure-test-infrastructure
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - vitest.config.mts
  - __tests__/mocks/server.ts
  - __tests__/mocks/handlers.ts
  - __tests__/mocks/fixtures.ts
  - __tests__/lib/scoring.test.ts
  - __tests__/lib/team-calibration.test.ts
  - __tests__/lib/chart-data.test.ts
  - package.json
autonomous: true

must_haves:
  truths:
    - "Running npx vitest executes unit tests and all pass"
    - "Coverage is reported for src/lib/** files"
    - "Scoring tests verify computeHealth with different team sizes and gap configurations"
    - "Team calibration tests verify tier classification and threshold lookups"
    - "Chart data tests verify computeHealthTrends with edge cases (empty, single, multi-period)"
    - "MSW handlers intercept Claude API calls at the network level"
  artifacts:
    - path: "vitest.config.mts"
      provides: "Vitest configuration with path aliases, node environment, coverage"
      contains: "defineConfig"
    - path: "__tests__/mocks/server.ts"
      provides: "MSW server instance for Vitest"
      contains: "setupServer"
    - path: "__tests__/mocks/handlers.ts"
      provides: "MSW request handlers intercepting api.anthropic.com"
      contains: "anthropic"
    - path: "__tests__/mocks/fixtures.ts"
      provides: "Reusable test data for decompose responses, steps, gaps"
      contains: "mockDecomposition"
    - path: "__tests__/lib/scoring.test.ts"
      provides: "Unit tests for computeHealth function"
      contains: "computeHealth"
    - path: "__tests__/lib/team-calibration.test.ts"
      provides: "Unit tests for getTeamTier and getThresholds"
      contains: "getTeamTier"
    - path: "__tests__/lib/chart-data.test.ts"
      provides: "Unit tests for computeHealthTrends"
      contains: "computeHealthTrends"
  key_links:
    - from: "vitest.config.mts"
      to: "tsconfig.json"
      via: "vite-tsconfig-paths resolves @/* alias"
      pattern: "tsconfigPaths"
    - from: "__tests__/mocks/server.ts"
      to: "__tests__/mocks/handlers.ts"
      via: "setupServer receives handlers array"
      pattern: "setupServer.*handlers"
    - from: "__tests__/lib/scoring.test.ts"
      to: "src/lib/scoring.ts"
      via: "imports computeHealth"
      pattern: "import.*computeHealth.*scoring"
---

<objective>
Set up Vitest test infrastructure with MSW mock layer and comprehensive unit tests for all pure business logic: scoring engine, team calibration, and chart data computation.

Purpose: Establish the test safety net that protects core business logic during all subsequent v1.1 work. MSW handlers provide the foundation for decompose pipeline tests in plan 05-03.
Output: Vitest config, MSW mock infrastructure (3 files), and 3 test suites covering scoring, calibration, and chart data.
</objective>

<execution_context>
@C:/Users/Brian/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Brian/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/lib/scoring.ts
@src/lib/team-calibration.ts
@src/lib/chart-data.ts
@src/lib/types.ts
@src/lib/decompose.ts
@src/lib/claude.ts
@tsconfig.json
@package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install test dependencies and configure Vitest with MSW mock layer</name>
  <files>package.json, vitest.config.mts, __tests__/mocks/server.ts, __tests__/mocks/handlers.ts, __tests__/mocks/fixtures.ts, __tests__/setup.ts</files>
  <action>
1. Install dev dependencies (single npm install command):
   `npm install -D vitest@latest @vitejs/plugin-react@latest @vitest/coverage-v8@latest vite-tsconfig-paths@latest msw@latest`

   Note: Do NOT install jsdom. We use `environment: "node"` for server-side logic tests. These are pure function tests that don't need DOM.

2. Create `vitest.config.mts` at project root:
   ```typescript
   import { defineConfig } from "vitest/config";
   import react from "@vitejs/plugin-react";
   import tsconfigPaths from "vite-tsconfig-paths";

   export default defineConfig({
     plugins: [react(), tsconfigPaths()],
     test: {
       environment: "node",
       globals: true,
       setupFiles: ["./__tests__/setup.ts"],
       include: ["__tests__/**/*.test.ts"],
       coverage: {
         provider: "v8",
         include: ["src/lib/**"],
         reporter: ["text", "text-summary"],
       },
     },
   });
   ```

3. Create `__tests__/setup.ts` (MSW server lifecycle):
   ```typescript
   import { beforeAll, afterEach, afterAll } from "vitest";
   import { server } from "./mocks/server";

   beforeAll(() => server.listen({ onUnhandledRequest: "bypass" }));
   afterEach(() => server.resetHandlers());
   afterAll(() => server.close());
   ```

4. Create `__tests__/mocks/handlers.ts`:
   Define a default MSW handler that intercepts POST requests to `https://api.anthropic.com/v1/messages`. The handler returns a properly formatted Anthropic Messages API response with a valid JSON decomposition in a code fence. Use the fixture data from fixtures.ts.

   ```typescript
   import { http, HttpResponse } from "msw";
   import { MOCK_DECOMPOSE_RESPONSE } from "./fixtures";

   export const handlers = [
     http.post("https://api.anthropic.com/v1/messages", () => {
       return HttpResponse.json({
         id: "msg_mock_001",
         type: "message",
         role: "assistant",
         model: "claude-sonnet-4-20250514",
         content: [
           {
             type: "text",
             text: "```json\n" + JSON.stringify(MOCK_DECOMPOSE_RESPONSE) + "\n```",
           },
         ],
         stop_reason: "end_turn",
         stop_sequence: null,
         usage: {
           input_tokens: 1500,
           output_tokens: 800,
         },
       });
     }),
   ];
   ```

5. Create `__tests__/mocks/server.ts`:
   ```typescript
   import { setupServer } from "msw/node";
   import { handlers } from "./handlers";

   export const server = setupServer(...handlers);
   ```

6. Create `__tests__/mocks/fixtures.ts`:
   Define reusable test data. Include:

   - `MOCK_STEPS`: Array of 3-4 Step objects with varied layers, automation scores, owners, and dependencies. Include at least one step with automationScore < 30 (for lowAutoSteps detection), varied owners for load balance testing.
   - `MOCK_GAPS`: Array of 3-4 Gap objects covering different types (bottleneck, single_dependency, manual_overhead) and severities (high, medium, low).
   - `MOCK_DECOMPOSE_RESPONSE`: A valid `{ title, steps, gaps }` object matching the DecompositionResponseSchema that the handler returns.
   - `MOCK_WORKFLOWS`: Array of Workflow objects with different createdAt dates (spanning 3+ weeks) for chart-data time-series testing. Each must have a decomposition with health scores.
   - `makeStep(overrides)`: Helper that returns a Step with sensible defaults, letting tests override specific fields.
   - `makeGap(overrides)`: Helper that returns a Gap with sensible defaults.
   - `makeWorkflow(overrides)`: Helper that returns a Workflow with sensible defaults including decomposition and health.

   All types imported from `@/lib/types`.

7. Add scripts to `package.json`:
   ```json
   "test": "vitest run",
   "test:watch": "vitest",
   "test:coverage": "vitest run --coverage"
   ```
  </action>
  <verify>Run `npx vitest run --passWithNoTests` to confirm Vitest starts and exits cleanly with the config. Run `npx tsc --noEmit` to verify no type errors in test files.</verify>
  <done>Vitest configured with node environment, path aliases, coverage on src/lib/**, MSW server lifecycle in setup.ts, mock handlers intercepting Anthropic API, and reusable test fixtures.</done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for scoring engine and team calibration</name>
  <files>__tests__/lib/scoring.test.ts, __tests__/lib/team-calibration.test.ts</files>
  <action>
1. Create `__tests__/lib/scoring.test.ts` testing `computeHealth` from `@/lib/scoring`:

   Test cases for computeHealth:
   a. **Empty inputs**: `computeHealth([], [])` returns all zeroes/defaults (complexity=0, fragility=0, automationPotential=0, teamLoadBalance uses medium baseline=60)
   b. **Basic computation**: Pass 3 steps with known automationScores, 2 gaps. Verify complexity formula: `min(100, stepCount*6 + depCount*3 + uniqueLayers*5)`. Verify fragility formula: `highGaps*20 + medGaps*10 + singleDepGaps*15 + lowAutoSteps*5` multiplied by fragilityMultiplier. Verify automationPotential is average of step automationScores.
   c. **Team size calibration - solo (1)**: Same steps/gaps, teamSize=1. Verify fragility is amplified by 1.8x vs medium baseline. Verify confidence is `{ level: "high", reason: "Team size was explicitly provided" }`.
   d. **Team size calibration - large (25)**: Same steps/gaps, teamSize=25. Verify fragility is reduced by 0.8x multiplier. Verify loadBalanceBaseline is 70.
   e. **No team size**: `computeHealth(steps, gaps)` without teamSize. Verify confidence is `{ level: "inferred" }`.
   f. **Load balance - single owner**: All steps have same owner. Verify teamLoadBalance is min of baseline and `100/ownerCount`.
   g. **Load balance - balanced**: Steps evenly distributed across 3 owners. Verify teamLoadBalance is high (near 100).
   h. **Score clamping**: Extreme inputs that would exceed 100. Verify all scores clamped to 0-100 range.

   Use `describe`/`it` blocks. Import `MOCK_STEPS` and `MOCK_GAPS` from fixtures but also create inline test-specific data for precise formula verification.

2. Create `__tests__/lib/team-calibration.test.ts` testing `getTeamTier` and `getThresholds` from `@/lib/team-calibration`:

   Test cases for getTeamTier:
   a. `getTeamTier(1)` returns "solo"
   b. `getTeamTier(0)` returns "solo" (edge: zero/negative)
   c. `getTeamTier(3)` returns "small"
   d. `getTeamTier(5)` returns "small" (boundary)
   e. `getTeamTier(6)` returns "medium" (boundary)
   f. `getTeamTier(20)` returns "medium" (boundary)
   g. `getTeamTier(21)` returns "large" (boundary)
   h. `getTeamTier(100)` returns "large"

   Test cases for getThresholds:
   a. `getThresholds(undefined)` returns medium thresholds (fragilityMultiplier=1.0)
   b. `getThresholds(null)` returns medium thresholds
   c. `getThresholds(1)` returns solo thresholds (fragilityMultiplier=1.8, bottleneckMultiplier=1.5, loadBalanceBaseline=30)
   d. `getThresholds(10)` returns medium thresholds (all 1.0)
   e. `getThresholds(50)` returns large thresholds (fragilityMultiplier=0.8)
  </action>
  <verify>Run `npx vitest run __tests__/lib/scoring.test.ts __tests__/lib/team-calibration.test.ts` -- all tests pass.</verify>
  <done>Scoring engine has 8+ test cases covering formula correctness, team calibration, load balancing, clamping, and confidence levels. Team calibration has 13+ test cases covering all tier boundaries and threshold lookups.</done>
</task>

<task type="auto">
  <name>Task 3: Unit tests for chart data computation</name>
  <files>__tests__/lib/chart-data.test.ts</files>
  <action>
Create `__tests__/lib/chart-data.test.ts` testing `computeHealthTrends` from `@/lib/chart-data`:

Test cases:
a. **Empty array**: `computeHealthTrends([])` returns `[]`
b. **No valid workflows**: Array of workflows missing decomposition or health. Returns `[]`.
c. **Single workflow**: One workflow returns one trend point with that workflow's exact health values. Verify all fields: date, label, count=1, complexity, fragility, automationPotential, teamLoadBalance, overallHealth formula.
d. **Two workflows same week**: Two workflows created 2 days apart in the same week. Returns one trend point with averaged health scores. Verify averaging: if workflow A has complexity=60 and B has complexity=80, trend shows complexity=70.
e. **Multi-week data**: 4 workflows spread across 3 different weeks. Returns 3 trend points sorted chronologically. Verify counts match per-week groupings.
f. **Month granularity**: Pass `granularity: "month"`. Verify grouping uses month buckets instead of weeks.
g. **Overall health formula**: Verify `overallHealth = round((complexity + (100 - fragility) + automationPotential + teamLoadBalance) / 4)`. This is the formula from chart-data.ts line 85-88.
h. **Date labels**: Verify the `label` field uses "Mon Day" format (e.g., "Jan 15").
i. **Week start calculation**: A workflow created on a Wednesday should be grouped into the Monday-start week bucket. Verify the date key is the preceding Monday.

Use `makeWorkflow` from fixtures to build test data, overriding `createdAt` and `decomposition.health` values for precise assertions.
  </action>
  <verify>Run `npx vitest run __tests__/lib/chart-data.test.ts` -- all tests pass. Run `npx vitest run --coverage` to see coverage output for src/lib/scoring.ts, src/lib/team-calibration.ts, src/lib/chart-data.ts.</verify>
  <done>Chart data computation has 9+ test cases covering empty inputs, single/multi workflows, week/month granularity, averaging correctness, overall health formula, and date formatting.</done>
</task>

</tasks>

<verification>
1. `npx vitest run` executes all tests and reports 0 failures
2. `npx vitest run --coverage` shows coverage for src/lib/scoring.ts, src/lib/team-calibration.ts, src/lib/chart-data.ts
3. `npx tsc --noEmit` passes (no type errors in test files or config)
4. MSW server starts and stops cleanly (no hanging processes)
5. Fixture data matches the Zod schemas used in decompose.ts
</verification>

<success_criteria>
- npx vitest run passes all tests (scoring, team-calibration, chart-data)
- Coverage is reported for src/lib/** with meaningful coverage on the three tested files
- MSW handlers are configured and intercepting https://api.anthropic.com/v1/messages
- All test fixtures are properly typed using the project's type definitions
</success_criteria>

<output>
After completion, create `.planning/phases/05-debt-closure-test-infrastructure/05-02-SUMMARY.md`
</output>
